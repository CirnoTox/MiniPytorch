{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"./python\")\n",
    "import mpt\n",
    "import numpy as np\n",
    "import time\n",
    "from tic_toc_timer import tic, toc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "data_dir=\"./data/\"\n",
    "batch_size=100\n",
    "train_dataset = mpt.data.MNISTDataset(\n",
    "    data_dir+\"/train-images-idx3-ubyte.gz\",\n",
    "    data_dir+\"/train-labels-idx1-ubyte.gz\"\n",
    ")\n",
    "test_dataset = mpt.data.MNISTDataset(\n",
    "    data_dir+\"/t10k-images-idx3-ubyte.gz\",\n",
    "    data_dir+\"/t10k-labels-idx1-ubyte.gz\"\n",
    ")\n",
    "train_dataloader = mpt.data.DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "test_dataloader = mpt.data.DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define network\n",
    "import mpt.nn as nn\n",
    "\n",
    "def simple_nn():\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(784, 20),\n",
    "        nn.Linear(20, 10)\n",
    "    )\n",
    "def loss_err(h, y):\n",
    "    \"\"\" Helper function to compute both loss and error\"\"\"\n",
    "    lossModule = nn.SoftmaxLoss()\n",
    "    return (\n",
    "        lossModule.forward(h, y),\n",
    "        np.sum(h.numpy().argmax(axis=1) != y.numpy(), dtype=np.float32)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#epoch\n",
    "def epoch(dataloader: mpt.data.DataLoader,\n",
    "          model: mpt.nn.Module,\n",
    "          opt: mpt.optim.Optimizer = None):\n",
    "    np.random.seed(4)\n",
    "\n",
    "    if opt is None:\n",
    "        model.eval()\n",
    "    else :\n",
    "        model.train()\n",
    "    loss = 0\n",
    "    err = 0\n",
    "    i = 0\n",
    "    num_sample = 0\n",
    "    for _, data in enumerate(dataloader):\n",
    "        imgs = data[0]\n",
    "        labels = data[1]\n",
    "        i += 1\n",
    "        forwardRes = model.forward(imgs)\n",
    "        iLoss, iError = loss_err(forwardRes, labels)\n",
    "        loss += iLoss.numpy()[0]\n",
    "        err += iError\n",
    "        num_sample += labels.shape[0]\n",
    "        if opt is not None:\n",
    "            iLoss.backward()\n",
    "            opt.step()\n",
    "        print(\"{:.2f}% |   {:.5f}\".format((err/num_sample)*100,loss/i))\n",
    "\n",
    "    return (err/num_sample, loss/i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Err | Loss\n",
      "93.00% |   2.50175\n",
      "83.50% |   2.33086\n",
      "79.33% |   2.24553\n",
      "74.75% |   2.15502\n",
      "71.20% |   2.07882\n",
      "67.67% |   2.00634\n",
      "63.86% |   1.93951\n",
      "61.00% |   1.88291\n",
      "57.78% |   1.79896\n",
      "55.60% |   1.75631\n",
      "53.73% |   1.71284\n",
      "52.17% |   1.67485\n",
      "50.23% |   1.63008\n",
      "49.07% |   1.60569\n",
      "47.53% |   1.56638\n",
      "46.00% |   1.53724\n",
      "45.06% |   1.50852\n",
      "44.11% |   1.48187\n",
      "43.37% |   1.45998\n",
      "42.20% |   1.43327\n",
      "41.33% |   1.41159\n",
      "40.91% |   1.39114\n",
      "40.39% |   1.37336\n",
      "39.50% |   1.34843\n",
      "38.88% |   1.32871\n",
      "37.92% |   1.30506\n",
      "37.26% |   1.28609\n",
      "36.71% |   1.27104\n",
      "36.28% |   1.25793\n",
      "35.70% |   1.24364\n",
      "35.16% |   1.22610\n",
      "34.78% |   1.21532\n",
      "34.33% |   1.20553\n",
      "33.91% |   1.19249\n",
      "33.46% |   1.17953\n",
      "33.00% |   1.16762\n",
      "32.76% |   1.15648\n",
      "32.21% |   1.14335\n",
      "31.79% |   1.13026\n",
      "31.52% |   1.11965\n",
      "31.15% |   1.10754\n",
      "30.83% |   1.09665\n",
      "30.35% |   1.08365\n",
      "30.11% |   1.07475\n",
      "29.76% |   1.06466\n",
      "29.37% |   1.05444\n",
      "29.26% |   1.04744\n",
      "28.94% |   1.03789\n",
      "28.69% |   1.02936\n",
      "28.50% |   1.02087\n",
      "28.37% |   1.01370\n",
      "28.12% |   1.00638\n",
      "27.89% |   0.99904\n",
      "27.70% |   0.99355\n",
      "27.49% |   0.98568\n",
      "27.27% |   0.97881\n",
      "27.09% |   0.97184\n",
      "26.84% |   0.96516\n",
      "26.64% |   0.95952\n",
      "26.50% |   0.95304\n",
      "26.38% |   0.94678\n",
      "26.18% |   0.93956\n",
      "26.13% |   0.93636\n",
      "26.02% |   0.93287\n",
      "25.88% |   0.92723\n",
      "25.77% |   0.92295\n",
      "25.51% |   0.91585\n",
      "25.38% |   0.91128\n",
      "25.17% |   0.90484\n",
      "25.03% |   0.89913\n",
      "24.93% |   0.89457\n",
      "24.86% |   0.89193\n",
      "24.77% |   0.88881\n",
      "24.59% |   0.88341\n",
      "24.47% |   0.87861\n",
      "24.32% |   0.87382\n",
      "24.26% |   0.87094\n",
      "24.14% |   0.86673\n",
      "24.03% |   0.86314\n",
      "24.00% |   0.86047\n",
      "23.86% |   0.85620\n",
      "23.76% |   0.85196\n",
      "23.66% |   0.84839\n",
      "23.62% |   0.84536\n",
      "23.49% |   0.84210\n",
      "23.48% |   0.83918\n",
      "23.36% |   0.83499\n",
      "23.32% |   0.83191\n",
      "23.26% |   0.82924\n",
      "23.19% |   0.82668\n",
      "23.10% |   0.82397\n",
      "22.99% |   0.82055\n",
      "22.85% |   0.81587\n",
      "22.70% |   0.81135\n",
      "22.62% |   0.80825\n",
      "22.51% |   0.80478\n",
      "22.34% |   0.79987\n",
      "22.22% |   0.79647\n",
      "22.17% |   0.79330\n",
      "22.06% |   0.78966\n",
      "21.94% |   0.78603\n",
      "21.88% |   0.78419\n",
      "21.79% |   0.78097\n",
      "21.70% |   0.77784\n",
      "21.63% |   0.77514\n",
      "21.58% |   0.77333\n",
      "21.49% |   0.76987\n",
      "21.48% |   0.76789\n",
      "21.38% |   0.76462\n",
      "21.35% |   0.76219\n",
      "21.26% |   0.75964\n",
      "21.21% |   0.75798\n",
      "21.11% |   0.75454\n",
      "21.01% |   0.75125\n",
      "21.00% |   0.75139\n",
      "20.90% |   0.74819\n",
      "20.82% |   0.74564\n",
      "20.72% |   0.74253\n",
      "20.66% |   0.74036\n",
      "20.53% |   0.73644\n",
      "20.45% |   0.73380\n",
      "20.39% |   0.73176\n",
      "20.35% |   0.73061\n",
      "20.27% |   0.72839\n",
      "20.26% |   0.72756\n",
      "20.17% |   0.72478\n",
      "20.09% |   0.72159\n",
      "20.02% |   0.71968\n",
      "19.93% |   0.71652\n",
      "19.88% |   0.71488\n",
      "19.84% |   0.71341\n",
      "19.79% |   0.71139\n",
      "19.74% |   0.70981\n",
      "19.67% |   0.70740\n",
      "19.57% |   0.70450\n",
      "19.51% |   0.70266\n",
      "19.46% |   0.70046\n",
      "19.41% |   0.69875\n",
      "19.36% |   0.69695\n",
      "19.29% |   0.69427\n",
      "19.23% |   0.69289\n",
      "19.20% |   0.69147\n",
      "19.17% |   0.68983\n",
      "19.13% |   0.68878\n",
      "19.10% |   0.68724\n",
      "19.10% |   0.68644\n",
      "19.05% |   0.68461\n",
      "18.99% |   0.68237\n",
      "18.97% |   0.68139\n",
      "18.95% |   0.68021\n",
      "18.89% |   0.67798\n",
      "18.84% |   0.67576\n",
      "18.82% |   0.67442\n",
      "18.77% |   0.67271\n",
      "18.73% |   0.67175\n",
      "18.72% |   0.67114\n",
      "18.66% |   0.66875\n",
      "18.61% |   0.66707\n",
      "18.55% |   0.66552\n",
      "18.56% |   0.66504\n",
      "18.58% |   0.66472\n",
      "18.55% |   0.66299\n",
      "18.50% |   0.66156\n",
      "18.41% |   0.65903\n",
      "18.35% |   0.65739\n",
      "18.31% |   0.65618\n",
      "18.25% |   0.65467\n",
      "18.23% |   0.65369\n",
      "18.20% |   0.65221\n",
      "18.15% |   0.65075\n",
      "18.12% |   0.65009\n",
      "18.09% |   0.64854\n",
      "18.06% |   0.64740\n",
      "18.04% |   0.64621\n",
      "18.02% |   0.64588\n",
      "17.99% |   0.64449\n",
      "17.93% |   0.64259\n",
      "17.88% |   0.64077\n",
      "17.84% |   0.63935\n",
      "17.81% |   0.63792\n",
      "17.79% |   0.63674\n",
      "17.80% |   0.63632\n",
      "17.77% |   0.63527\n",
      "17.77% |   0.63430\n",
      "17.75% |   0.63309\n",
      "17.70% |   0.63150\n",
      "17.66% |   0.62990\n",
      "17.64% |   0.62882\n",
      "17.58% |   0.62698\n",
      "17.54% |   0.62548\n",
      "17.48% |   0.62358\n",
      "17.43% |   0.62188\n",
      "17.40% |   0.62131\n",
      "17.34% |   0.61951\n",
      "17.31% |   0.61822\n",
      "17.30% |   0.61741\n",
      "17.28% |   0.61663\n",
      "17.25% |   0.61583\n",
      "17.20% |   0.61411\n",
      "17.14% |   0.61247\n",
      "17.12% |   0.61137\n",
      "17.07% |   0.60975\n",
      "17.03% |   0.60906\n",
      "17.01% |   0.60794\n",
      "16.97% |   0.60632\n",
      "16.94% |   0.60527\n",
      "16.90% |   0.60469\n",
      "16.88% |   0.60399\n",
      "16.84% |   0.60247\n",
      "16.84% |   0.60195\n",
      "16.83% |   0.60123\n",
      "16.83% |   0.60088\n",
      "16.82% |   0.60050\n",
      "16.78% |   0.59937\n",
      "16.78% |   0.59897\n",
      "16.76% |   0.59822\n",
      "16.73% |   0.59696\n",
      "16.69% |   0.59593\n",
      "16.68% |   0.59556\n",
      "16.64% |   0.59429\n",
      "16.62% |   0.59324\n",
      "16.57% |   0.59201\n",
      "16.53% |   0.59091\n",
      "16.50% |   0.58967\n",
      "16.49% |   0.58892\n",
      "16.46% |   0.58775\n",
      "16.41% |   0.58606\n",
      "16.38% |   0.58525\n",
      "16.38% |   0.58457\n",
      "16.35% |   0.58360\n",
      "16.32% |   0.58254\n",
      "16.29% |   0.58171\n",
      "16.24% |   0.58014\n",
      "16.22% |   0.57934\n",
      "16.17% |   0.57821\n",
      "16.14% |   0.57711\n",
      "16.13% |   0.57679\n",
      "16.10% |   0.57555\n",
      "16.08% |   0.57463\n",
      "16.07% |   0.57417\n",
      "16.08% |   0.57459\n",
      "16.08% |   0.57384\n",
      "16.08% |   0.57405\n",
      "16.07% |   0.57347\n",
      "16.05% |   0.57268\n",
      "16.02% |   0.57161\n",
      "16.00% |   0.57077\n",
      "15.98% |   0.57042\n",
      "15.96% |   0.56961\n",
      "15.91% |   0.56813\n",
      "15.90% |   0.56840\n",
      "15.87% |   0.56754\n",
      "15.87% |   0.56683\n",
      "15.85% |   0.56601\n",
      "15.80% |   0.56481\n",
      "15.80% |   0.56432\n",
      "15.76% |   0.56328\n",
      "15.76% |   0.56259\n",
      "15.73% |   0.56173\n",
      "15.72% |   0.56097\n",
      "15.70% |   0.56002\n",
      "15.68% |   0.55915\n",
      "15.64% |   0.55812\n",
      "15.64% |   0.55793\n",
      "15.63% |   0.55770\n",
      "15.62% |   0.55711\n",
      "15.59% |   0.55592\n",
      "15.59% |   0.55509\n",
      "15.58% |   0.55457\n",
      "15.55% |   0.55348\n",
      "15.51% |   0.55247\n",
      "15.50% |   0.55191\n",
      "15.48% |   0.55157\n",
      "15.47% |   0.55125\n",
      "15.44% |   0.55012\n",
      "15.41% |   0.54914\n",
      "15.40% |   0.54872\n",
      "15.39% |   0.54827\n",
      "15.39% |   0.54779\n",
      "15.37% |   0.54725\n",
      "15.36% |   0.54727\n",
      "15.35% |   0.54653\n",
      "15.34% |   0.54616\n",
      "15.32% |   0.54549\n",
      "15.31% |   0.54460\n",
      "15.29% |   0.54429\n",
      "15.28% |   0.54331\n",
      "15.26% |   0.54249\n",
      "15.24% |   0.54219\n",
      "15.22% |   0.54122\n",
      "15.20% |   0.54044\n",
      "15.17% |   0.53957\n",
      "15.15% |   0.53925\n",
      "15.13% |   0.53839\n",
      "15.12% |   0.53771\n",
      "15.08% |   0.53668\n",
      "15.05% |   0.53585\n",
      "15.01% |   0.53476\n",
      "15.01% |   0.53417\n",
      "14.99% |   0.53361\n",
      "14.98% |   0.53326\n",
      "14.99% |   0.53333\n",
      "14.97% |   0.53259\n",
      "14.93% |   0.53155\n",
      "14.91% |   0.53079\n",
      "14.90% |   0.53035\n",
      "14.89% |   0.53003\n",
      "14.86% |   0.52900\n",
      "14.83% |   0.52815\n",
      "14.81% |   0.52736\n",
      "14.79% |   0.52661\n",
      "14.78% |   0.52579\n",
      "14.76% |   0.52518\n",
      "14.73% |   0.52446\n",
      "14.71% |   0.52404\n",
      "14.71% |   0.52394\n",
      "14.71% |   0.52378\n",
      "14.72% |   0.52335\n",
      "14.70% |   0.52278\n",
      "14.67% |   0.52223\n",
      "14.67% |   0.52197\n",
      "14.65% |   0.52124\n",
      "14.63% |   0.52072\n",
      "14.63% |   0.52045\n",
      "14.62% |   0.51971\n",
      "14.62% |   0.51926\n",
      "14.60% |   0.51853\n",
      "14.59% |   0.51795\n",
      "14.56% |   0.51698\n",
      "14.54% |   0.51624\n",
      "14.53% |   0.51588\n",
      "14.54% |   0.51570\n",
      "14.51% |   0.51481\n",
      "14.49% |   0.51422\n",
      "14.47% |   0.51342\n",
      "14.46% |   0.51291\n",
      "14.45% |   0.51254\n",
      "14.43% |   0.51230\n",
      "14.43% |   0.51202\n",
      "14.41% |   0.51158\n",
      "14.39% |   0.51110\n",
      "14.39% |   0.51075\n",
      "14.38% |   0.51019\n",
      "14.37% |   0.50957\n",
      "14.36% |   0.50946\n",
      "14.35% |   0.50909\n",
      "14.34% |   0.50868\n",
      "14.33% |   0.50838\n",
      "14.32% |   0.50806\n",
      "14.31% |   0.50768\n",
      "14.29% |   0.50695\n",
      "14.28% |   0.50622\n",
      "14.27% |   0.50589\n",
      "14.28% |   0.50642\n",
      "14.28% |   0.50671\n",
      "14.26% |   0.50598\n",
      "14.28% |   0.50613\n",
      "14.26% |   0.50566\n",
      "14.25% |   0.50552\n",
      "14.24% |   0.50492\n",
      "14.24% |   0.50468\n",
      "14.24% |   0.50440\n",
      "14.22% |   0.50390\n",
      "14.21% |   0.50353\n",
      "14.19% |   0.50335\n",
      "14.19% |   0.50293\n",
      "14.18% |   0.50237\n",
      "14.18% |   0.50214\n",
      "14.16% |   0.50150\n",
      "14.16% |   0.50148\n",
      "14.15% |   0.50087\n",
      "14.12% |   0.50018\n",
      "14.11% |   0.49941\n",
      "14.10% |   0.49899\n",
      "14.08% |   0.49839\n",
      "14.09% |   0.49837\n",
      "14.06% |   0.49766\n",
      "14.06% |   0.49710\n",
      "14.06% |   0.49725\n",
      "14.06% |   0.49738\n",
      "14.05% |   0.49700\n",
      "14.05% |   0.49699\n",
      "14.03% |   0.49646\n",
      "14.02% |   0.49606\n",
      "14.02% |   0.49569\n",
      "14.03% |   0.49595\n",
      "14.01% |   0.49540\n",
      "14.00% |   0.49498\n",
      "14.00% |   0.49482\n",
      "13.98% |   0.49447\n",
      "13.96% |   0.49392\n",
      "13.95% |   0.49343\n",
      "13.93% |   0.49282\n",
      "13.91% |   0.49232\n",
      "13.89% |   0.49164\n",
      "13.88% |   0.49147\n",
      "13.87% |   0.49109\n",
      "13.86% |   0.49062\n",
      "13.85% |   0.49012\n",
      "13.85% |   0.48992\n",
      "13.83% |   0.48937\n",
      "13.82% |   0.48894\n",
      "13.82% |   0.48892\n",
      "13.81% |   0.48846\n",
      "13.80% |   0.48790\n",
      "13.79% |   0.48750\n",
      "13.79% |   0.48723\n",
      "13.78% |   0.48694\n",
      "13.78% |   0.48665\n",
      "13.76% |   0.48624\n",
      "13.75% |   0.48600\n",
      "13.75% |   0.48601\n",
      "13.73% |   0.48568\n",
      "13.72% |   0.48500\n",
      "13.71% |   0.48466\n",
      "13.70% |   0.48418\n",
      "13.71% |   0.48401\n",
      "13.70% |   0.48379\n",
      "13.69% |   0.48352\n",
      "13.68% |   0.48309\n",
      "13.67% |   0.48291\n",
      "13.67% |   0.48296\n",
      "13.67% |   0.48289\n",
      "13.66% |   0.48261\n",
      "13.64% |   0.48221\n",
      "13.63% |   0.48180\n",
      "13.62% |   0.48133\n",
      "13.61% |   0.48089\n",
      "13.60% |   0.48050\n",
      "13.59% |   0.48035\n",
      "13.58% |   0.48011\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mErr | Loss\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m10\u001b[39m):\n\u001b[0;32m----> 7\u001b[0m     train_acc, train_loss \u001b[39m=\u001b[39m epoch(train_dataloader, model\u001b[39m=\u001b[39;49mmodel, opt\u001b[39m=\u001b[39;49mopt)\n\u001b[1;32m      8\u001b[0m test_acc, test_loss \u001b[39m=\u001b[39m epoch(test_dataloader, model\u001b[39m=\u001b[39mmodel)\n\u001b[1;32m      9\u001b[0m \u001b[39mprint\u001b[39m()\n",
      "Cell \u001b[0;32mIn[10], line 25\u001b[0m, in \u001b[0;36mepoch\u001b[0;34m(dataloader, model, opt)\u001b[0m\n\u001b[1;32m     23\u001b[0m num_sample \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[1;32m     24\u001b[0m \u001b[39mif\u001b[39;00m opt \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m     iLoss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     26\u001b[0m     opt\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     27\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m{:.2f}\u001b[39;00m\u001b[39m%\u001b[39m\u001b[39m |   \u001b[39m\u001b[39m{:.5f}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat((err\u001b[39m/\u001b[39mnum_sample)\u001b[39m*\u001b[39m\u001b[39m100\u001b[39m,loss\u001b[39m/\u001b[39mi))\n",
      "File \u001b[0;32m~/Desktop/finalProject/MiniPytorch/./python/mpt/autograd.py:289\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, out_grad)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbackward\u001b[39m(\u001b[39mself\u001b[39m, out_grad\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    284\u001b[0m     out_grad \u001b[39m=\u001b[39m (\n\u001b[1;32m    285\u001b[0m         out_grad\n\u001b[1;32m    286\u001b[0m         \u001b[39mif\u001b[39;00m out_grad\n\u001b[1;32m    287\u001b[0m         \u001b[39melse\u001b[39;00m init\u001b[39m.\u001b[39mones(\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshape, dtype\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdtype, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    288\u001b[0m     )\n\u001b[0;32m--> 289\u001b[0m     compute_gradient_of_variables(\u001b[39mself\u001b[39;49m, out_grad)\n",
      "File \u001b[0;32m~/Desktop/finalProject/MiniPytorch/./python/mpt/autograd.py:393\u001b[0m, in \u001b[0;36mcompute_gradient_of_variables\u001b[0;34m(output_tensor, out_grad)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[39mif\u001b[39;00m nodeI\u001b[39m.\u001b[39mop \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    392\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m--> 393\u001b[0m opgrad\u001b[39m=\u001b[39mnodeI\u001b[39m.\u001b[39;49mop\u001b[39m.\u001b[39;49mgradient(nodeI\u001b[39m.\u001b[39;49mgrad, nodeI)\n\u001b[1;32m    394\u001b[0m \u001b[39mfor\u001b[39;00m node_j, grad_j \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(nodeI\u001b[39m.\u001b[39minputs, as_tuple(opgrad)):\n\u001b[1;32m    395\u001b[0m     \u001b[39mif\u001b[39;00m node_j \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m node_to_output_grads_list:\n",
      "File \u001b[0;32m~/Desktop/finalProject/MiniPytorch/./python/mpt/ops.py:291\u001b[0m, in \u001b[0;36mMatMul.gradient\u001b[0;34m(self, out_grad, node)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgradient\u001b[39m(\u001b[39mself\u001b[39m, out_grad, node):\n\u001b[1;32m    289\u001b[0m     \u001b[39m# https://github.com/bettersemut/dlsys_hw2/blob/8b16e4ecac6cf5d5efb2c4840f9107cdfe64e00b/python/needle/ops.py#L273\u001b[39;00m\n\u001b[1;32m    290\u001b[0m     lhs, rhs \u001b[39m=\u001b[39m node\u001b[39m.\u001b[39minputs\n\u001b[0;32m--> 291\u001b[0m     lhs_grad \u001b[39m=\u001b[39m out_grad\u001b[39m@rhs\u001b[39;49m\u001b[39m.\u001b[39;49mtranspose()\n\u001b[1;32m    292\u001b[0m     rhs_grad \u001b[39m=\u001b[39m lhs\u001b[39m.\u001b[39mtranspose()\u001b[39m@out_grad\u001b[39m\n\u001b[1;32m    293\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(lhs_grad\u001b[39m.\u001b[39mshape) \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(lhs\u001b[39m.\u001b[39mshape):\n",
      "File \u001b[0;32m~/Desktop/finalProject/MiniPytorch/./python/mpt/autograd.py:331\u001b[0m, in \u001b[0;36mTensor.__matmul__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__matmul__\u001b[39m(\u001b[39mself\u001b[39m, other):\n\u001b[0;32m--> 331\u001b[0m     \u001b[39mreturn\u001b[39;00m mpt\u001b[39m.\u001b[39;49mops\u001b[39m.\u001b[39;49mMatMul()(\u001b[39mself\u001b[39;49m, other)\n",
      "File \u001b[0;32m~/Desktop/finalProject/MiniPytorch/./python/mpt/autograd.py:66\u001b[0m, in \u001b[0;36mTensorOp.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs):\n\u001b[0;32m---> 66\u001b[0m     \u001b[39mreturn\u001b[39;00m Tensor\u001b[39m.\u001b[39;49mmake_from_op(\u001b[39mself\u001b[39;49m, args)\n",
      "File \u001b[0;32m~/Desktop/finalProject/MiniPytorch/./python/mpt/autograd.py:235\u001b[0m, in \u001b[0;36mTensor.make_from_op\u001b[0;34m(op, inputs)\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m tensor\u001b[39m.\u001b[39mrequires_grad:\n\u001b[1;32m    234\u001b[0m         \u001b[39mreturn\u001b[39;00m tensor\u001b[39m.\u001b[39mdetach()\n\u001b[0;32m--> 235\u001b[0m     tensor\u001b[39m.\u001b[39;49mrealize_cached_data()\n\u001b[1;32m    236\u001b[0m \u001b[39mreturn\u001b[39;00m tensor\n",
      "File \u001b[0;32m~/Desktop/finalProject/MiniPytorch/./python/mpt/autograd.py:93\u001b[0m, in \u001b[0;36mValue.realize_cached_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcached_data\n\u001b[1;32m     92\u001b[0m \u001b[39m# note: data implicitly calls realized cached data\u001b[39;00m\n\u001b[0;32m---> 93\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcached_data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mop\u001b[39m.\u001b[39;49mcompute(\n\u001b[1;32m     94\u001b[0m     \u001b[39m*\u001b[39;49m[x\u001b[39m.\u001b[39;49mrealize_cached_data() \u001b[39mfor\u001b[39;49;00m x \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minputs]\n\u001b[1;32m     95\u001b[0m )\n\u001b[1;32m     96\u001b[0m \u001b[39m# self.cached_data\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcached_data\n",
      "File \u001b[0;32m~/Desktop/finalProject/MiniPytorch/./python/mpt/ops.py:286\u001b[0m, in \u001b[0;36mMatMul.compute\u001b[0;34m(self, a, b)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute\u001b[39m(\u001b[39mself\u001b[39m, a, b):\n\u001b[0;32m--> 286\u001b[0m     \u001b[39mreturn\u001b[39;00m a\u001b[39m@b\u001b[39;49m\n",
      "File \u001b[0;32m~/Desktop/finalProject/MiniPytorch/./python/mpt/backend_ndarray/ndarray.py:563\u001b[0m, in \u001b[0;36mNDArray.__matmul__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    562\u001b[0m     out \u001b[39m=\u001b[39m NDArray\u001b[39m.\u001b[39mmake((m, p), device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m--> 563\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice\u001b[39m.\u001b[39;49mmatmul(\n\u001b[1;32m    564\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompact()\u001b[39m.\u001b[39;49m_handle, other\u001b[39m.\u001b[39;49mcompact()\u001b[39m.\u001b[39;49m_handle, out\u001b[39m.\u001b[39;49m_handle, m, n, p\n\u001b[1;32m    565\u001b[0m     )\n\u001b[1;32m    566\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/Desktop/finalProject/MiniPytorch/./python/mpt/backend_ndarray/ndarray_backend_numpy.py:109\u001b[0m, in \u001b[0;36mmatmul\u001b[0;34m(a, b, out, m, n, p)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmatmul\u001b[39m(a, b, out, m, n, p):\n\u001b[0;32m--> 109\u001b[0m     out\u001b[39m.\u001b[39marray[:] \u001b[39m=\u001b[39m (a\u001b[39m.\u001b[39;49marray\u001b[39m.\u001b[39;49mreshape(m, n) \u001b[39m@\u001b[39;49m b\u001b[39m.\u001b[39;49marray\u001b[39m.\u001b[39;49mreshape(n, p))\u001b[39m.\u001b[39;49mreshape(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train\n",
    "train_acc, train_loss = None, None\n",
    "model = simple_nn()\n",
    "opt = mpt.optim.SGD(model.parameters(), lr=0.1, weight_decay=0.001)\n",
    "print(\"Err | Loss\")\n",
    "for _ in range(10):\n",
    "    train_acc, train_loss = epoch(train_dataloader, model=model, opt=opt)\n",
    "test_acc, test_loss = epoch(test_dataloader, model=model)\n",
    "print()\n",
    "print(\"{:.5f} |   {:.5f}\".format(test_acc, test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "testImg=mpt.Tensor(test_dataset[0][0]).reshape((-1,1)).transpose()\n",
    "testLabel=mpt.Tensor(test_dataset[0][1])\n",
    "def loss_err(h, y):\n",
    "    lossModule = nn.SoftmaxLoss()\n",
    "    return (\n",
    "        lossModule.forward(h, y),\n",
    "        np.sum(h.numpy().argmax(axis=1) != y.numpy(), dtype=np.float32)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "h=model.forward(testImg)\n",
    "y=testLabel\n",
    "print(np.sum(h.numpy().argmax(axis=1) != y.numpy(), dtype=np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.38178068 -0.45340216  0.46996555  0.6675435  -0.31004506  0.46440935\n",
      "  -0.59473586  0.2853446  -1.3865604  -0.27761897]]\n",
      "[ True]\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "h2=net.forward(testImg)\n",
    "y2=testLabel\n",
    "print(h2)\n",
    "print(h2.numpy().argmax(axis=1) != y2.numpy())\n",
    "print(np.sum(h2.numpy().argmax(axis=1) != y2.numpy(), dtype=np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total time elapsed is 0:00:00.908593\n",
      "[[  2.8811848 -12.453715   -4.7072725 ... -13.744873  -20.835342\n",
      "   -8.44306  ]\n",
      " [-47.7621     -3.5861657   2.5874667 ...  -1.7962167  13.986323\n",
      "  -16.019613 ]\n",
      " [ 37.928497   20.674397  -14.926062  ...   9.745806   -5.121373\n",
      "   12.80354  ]\n",
      " ...\n",
      " [ -5.649159   10.830176    9.132078  ...  -3.7227452 -12.196058\n",
      "  -17.02923  ]\n",
      " [ 15.431809   11.98895    10.8527155 ... -13.062768   -7.806604\n",
      "   14.733793 ]\n",
      " [-17.335762   30.071396  -11.187022  ...   8.875761   27.101534\n",
      "   14.943199 ]] cpu() float32\n",
      "The total time elapsed is 0:00:19.478694\n"
     ]
    }
   ],
   "source": [
    "\n",
    "a=mpt.Tensor(np.random.randn(320,320),device=mpt.cpu())\n",
    "b=mpt.Tensor(np.random.randn(320,320),device=mpt.cpu())\n",
    "tic()\n",
    "res=mpt.ops.matmul(a,b)\n",
    "print(f\"The total time elapsed is {toc()}\")\n",
    "print(res,res.device,res.dtype)\n",
    "\n",
    "c=np.random.randn(320,320).astype('f')\n",
    "d=np.random.randn(320,320).astype('f')\n",
    "res2=np.ndarray((320,320))\n",
    "tic()\n",
    "for i in range(320):\n",
    "    for j in range(320):\n",
    "        thissum=0\n",
    "        for k in range(320):\n",
    "            thissum+=c[i,j]*d[j,k]\n",
    "        res2[i,k]=thissum\n",
    "# print(c)\n",
    "# print(d)\n",
    "print(f\"The total time elapsed is {toc()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4e1d9a8909477db77738c33245c29c7265277ef753467dede8cf3f814cde494e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
